{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers datasets nltk tqdm rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:13.121890Z","iopub.execute_input":"2024-12-16T13:32:13.122602Z","iopub.status.idle":"2024-12-16T13:32:22.745323Z","shell.execute_reply.started":"2024-12-16T13:32:13.122559Z","shell.execute_reply":"2024-12-16T13:32:22.744466Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nCollecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq,GenerationConfig\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport multiprocessing\nfrom tqdm import tqdm\nfrom rouge import Rouge\n\nimport multiprocessing\nmultiprocessing.set_start_method(\"forkserver\", force=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:22.747416Z","iopub.execute_input":"2024-12-16T13:32:22.748172Z","iopub.status.idle":"2024-12-16T13:32:41.195603Z","shell.execute_reply.started":"2024-12-16T13:32:22.748109Z","shell.execute_reply":"2024-12-16T13:32:41.194672Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define contraction mapping (you can expand this as needed)\ncontraction_mapping = {\n    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n    \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'll\": \"I will\",\n    \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n    \"mightn't\": \"might not\", \"might've\": \"might have\", \"mustn't\": \"must not\", \"must've\": \"must have\",\n    \"needn't\": \"need not\", \"o'clock\": \"of the clock\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n    \"that'd\": \"that would\", \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\",\n    \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\",\n    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what's\": \"what is\", \"where's\": \"where is\",\n    \"who's\": \"who is\", \"won't\": \"will not\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"y'all\": \"you all\",\n    \"you're\": \"you are\", \"you've\": \"you have\"\n}\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:41.196839Z","iopub.execute_input":"2024-12-16T13:32:41.197427Z","iopub.status.idle":"2024-12-16T13:32:41.373231Z","shell.execute_reply.started":"2024-12-16T13:32:41.197398Z","shell.execute_reply":"2024-12-16T13:32:41.372346Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Function to preprocess and clean text\ndef preprocess_and_clean(text):\n    text = re.sub(r'[^A-Za-z0-9]', ' ', text)  # Remove non-alphanumeric characters\n    text = text.lower()  # Convert text to lowercase\n    text = ' '.join([contraction_mapping.get(t, t) for t in text.split()])  # Expand contractions\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n    return ' '.join(tokens)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:41.374938Z","iopub.execute_input":"2024-12-16T13:32:41.375233Z","iopub.status.idle":"2024-12-16T13:32:41.380308Z","shell.execute_reply.started":"2024-12-16T13:32:41.375207Z","shell.execute_reply":"2024-12-16T13:32:41.379519Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Function to preprocess a dataset in parallel\ndef preprocess_dataset(dataset):\n    num_cpus = multiprocessing.cpu_count()\n    pool = multiprocessing.Pool(processes=num_cpus)\n    processed_texts = []\n    with tqdm(total=len(dataset), desc=\"Preprocessing\") as pbar:\n        for cleaned_text in pool.imap_unordered(preprocess_and_clean, dataset):\n            processed_texts.append(cleaned_text)\n            pbar.update()\n    pool.close()\n    pool.join()\n    return processed_texts","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:41.381233Z","iopub.execute_input":"2024-12-16T13:32:41.381477Z","iopub.status.idle":"2024-12-16T13:32:41.395053Z","shell.execute_reply.started":"2024-12-16T13:32:41.381453Z","shell.execute_reply":"2024-12-16T13:32:41.394270Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the dataset (for example, CNN/Daily Mail)\ndataset = load_dataset('cnn_dailymail', '3.0.0')\n\n# Limit the dataset to a smaller subset (e.g., first 1000 examples)\nlimited_train_dataset = dataset['train']\nlimited_eval_dataset = dataset['validation']\nlimited_test_dataset = dataset['test'].select(range(500))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:41.395988Z","iopub.execute_input":"2024-12-16T13:32:41.396272Z","iopub.status.idle":"2024-12-16T13:32:55.309114Z","shell.execute_reply.started":"2024-12-16T13:32:41.396243Z","shell.execute_reply":"2024-12-16T13:32:55.308494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"839e42d7f4334e438ba37c343a7ad69b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68b13ac7b0244509d9ebf29346aa9c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09705a6539ba4d3390d30d15b158ae86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032a952354084a9ab4fecf7d18488c70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e100c8aa644cf98921ad1ecfa6704d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f8ed911d544f94b78d393c49c06f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"114b19d44c1f457a93d50a04dc33bc38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"057580cacbc646d296b0ec207aa7fd6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c866517bc9647c4848a4d284513c0b8"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Tokenize the dataset for the BART model\ndef preprocess_data(examples):\n    model_inputs = bart_tokenizer(\n        examples['article'], max_length=1024, truncation=True, padding=\"max_length\"\n    )\n    \n    labels = bart_tokenizer(\n            examples['highlights'], max_length=128, truncation=True, padding=\"max_length\"\n        )\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:55.310082Z","iopub.execute_input":"2024-12-16T13:32:55.310394Z","iopub.status.idle":"2024-12-16T13:32:55.314832Z","shell.execute_reply.started":"2024-12-16T13:32:55.310363Z","shell.execute_reply":"2024-12-16T13:32:55.313937Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Initialize the BART tokenizer\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n# Apply preprocessing\ntokenized_datasets = {\n    'train': limited_train_dataset.map(preprocess_data, batched=True),\n    'validation': limited_eval_dataset.map(preprocess_data, batched=True),\n    'test': limited_test_dataset.map(preprocess_data, batched=True)\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:32:55.315864Z","iopub.execute_input":"2024-12-16T13:32:55.316190Z","iopub.status.idle":"2024-12-16T13:55:10.280330Z","shell.execute_reply.started":"2024-12-16T13:32:55.316163Z","shell.execute_reply":"2024-12-16T13:55:10.279524Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6327c2a41c44a3c84a4516a9c24337d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1990f61f5e73464d89f0f00678a695db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dca242613d34df89e38a84dadc59441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92488015b4474e829ac61edf4e7a0a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0508f373f37b4dd29013426204e57aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b29df9bb2674acda09c385a265f6efd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b7b9af77f74d47bef754f92d772ee8"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Load the BART model\nbart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(torch.cuda.is_available())  # Check if GPU is available\nbart_model.to(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:55:10.281460Z","iopub.execute_input":"2024-12-16T13:55:10.281823Z","iopub.status.idle":"2024-12-16T13:55:20.279610Z","shell.execute_reply.started":"2024-12-16T13:55:10.281783Z","shell.execute_reply":"2024-12-16T13:55:20.278737Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7600e0e064548b5bd478a31f1b2331e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd933925a87e4bea9549d1ada70ad0a5"}},"metadata":{}},{"name":"stdout","text":"True\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Data collator for the model\ndata_collator = DataCollatorForSeq2Seq(tokenizer=bart_tokenizer, model=bart_model, pad_to_multiple_of=8)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./bart_finetuned',\n    eval_strategy=\"steps\",\n    learning_rate=5e-5,  # Keep the learning rate the same\n    per_device_train_batch_size=1,  # Keeping this as 1 for memory efficiency\n    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n    per_device_eval_batch_size=1,\n    save_steps=500,  # Save less frequently to reduce I/O time\n    max_steps=500,  # Reduced the number of steps to speed up training\n    num_train_epochs=1,  # Set epochs to 1 for faster training\n    save_total_limit=2,\n    logging_dir='./logs',\n    logging_steps=100,  # Keep the logging frequency to monitor training\n    save_strategy=\"epoch\",  # Save at the end of each epoch\n    fp16=True,  # Mixed precision training for faster training\n    report_to=\"none\",\n    no_cuda=False if torch.cuda.is_available() else True,\n    tpu_num_cores=None,\n    dataloader_num_workers=4,  # Number of workers for faster data loading\n    lr_scheduler_type='linear',  # Apply linear learning rate scheduler\n    warmup_steps=200,  # Reduced warmup steps for quicker training\n)\n\n\n# Trainer for fine-tuning\ntrainer = Trainer(\n    model=bart_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'].shuffle(seed=42).select(range(10000)),\n    eval_dataset=tokenized_datasets['validation'].shuffle(seed=42).select(range(1000)),\n    processing_class=bart_tokenizer,\n    data_collator=data_collator,\n)\n# Define generation configuration\ngeneration_config = GenerationConfig(\n    max_length=142,\n    min_length=50,\n    early_stopping=True,\n    num_beams=4,\n    length_penalty=2.0,\n    no_repeat_ngram_size=3,\n    forced_bos_token_id=0,\n)\n\n# Train the model\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:55:20.282660Z","iopub.execute_input":"2024-12-16T13:55:20.282915Z","iopub.status.idle":"2024-12-16T14:14:37.442895Z","shell.execute_reply.started":"2024-12-16T13:55:20.282891Z","shell.execute_reply":"2024-12-16T14:14:37.441858Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 18:46, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.323200</td>\n      <td>0.935004</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.779900</td>\n      <td>0.984972</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.830300</td>\n      <td>0.934897</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.767200</td>\n      <td>0.890566</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.743400</td>\n      <td>0.869061</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.2887909698486328, metrics={'train_runtime': 1155.9947, 'train_samples_per_second': 0.865, 'train_steps_per_second': 0.433, 'total_flos': 2167104602112000.0, 'train_loss': 1.2887909698486328, 'epoch': 0.1})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Evaluate the model\nprint(\"Evaluating the model...\")\n\n# Create a DataLoader for the test set\nbatch_size = 8  # Adjust based on your GPU memory\ntest_loader = DataLoader(tokenized_datasets['test']['article'], batch_size=batch_size)\n\npredicted_summaries = []\n\nfor batch in test_loader:\n    # Tokenize and move to device\n    inputs = bart_tokenizer(\n        batch, return_tensors=\"pt\", max_length=512, truncation=True, padding=True\n    )\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    # Generate summaries\n    summary_ids = bart_model.generate(\n        inputs['input_ids'], attention_mask=inputs['attention_mask'],\n        generation_config=generation_config\n    )\n    \n    # Decode and append summaries\n    predicted_summaries.extend(bart_tokenizer.batch_decode(summary_ids, skip_special_tokens=True))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:14:37.444213Z","iopub.execute_input":"2024-12-16T14:14:37.444925Z","iopub.status.idle":"2024-12-16T14:19:36.101578Z","shell.execute_reply.started":"2024-12-16T14:14:37.444880Z","shell.execute_reply":"2024-12-16T14:19:36.100626Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Calculate ROUGE scores\nrouge = Rouge()\nscores = rouge.get_scores(predicted_summaries, tokenized_datasets['test']['highlights'], avg=True)\nprint(\"ROUGE Scores:\", scores)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:19:36.102667Z","iopub.execute_input":"2024-12-16T14:19:36.102912Z","iopub.status.idle":"2024-12-16T14:19:36.879836Z","shell.execute_reply.started":"2024-12-16T14:19:36.102888Z","shell.execute_reply":"2024-12-16T14:19:36.878935Z"}},"outputs":[{"name":"stdout","text":"ROUGE Scores: {'rouge-1': {'r': 0.40531385406904685, 'p': 0.25765859422834314, 'f': 0.3090104882724774}, 'rouge-2': {'r': 0.16261878606423094, 'p': 0.09542072287746495, 'f': 0.1176961824102381}, 'rouge-l': {'r': 0.3768242519603926, 'p': 0.23899180086990043, 'f': 0.28688761201064555}}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Save the fine-tuned model\nbart_model.save_pretrained('./bart_finetuned')\nbart_tokenizer.save_pretrained('./bart_finetuned')\n\nprint(\"Fine-tuned model saved in './bart_finetuned'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:19:36.880864Z","iopub.execute_input":"2024-12-16T14:19:36.881205Z","iopub.status.idle":"2024-12-16T14:19:40.434639Z","shell.execute_reply.started":"2024-12-16T14:19:36.881170Z","shell.execute_reply":"2024-12-16T14:19:40.433740Z"}},"outputs":[{"name":"stdout","text":"Fine-tuned model saved in './bart_finetuned'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# User-defined function for summarization\ndef summarize_text(text, num_words):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    bart_model.to(device) \n    min_length = min(30, num_words // 2)  \n    max_length = num_words\n    inputs = bart_tokenizer(text, return_tensors='pt', max_length=1024, truncation=True).to(device)\n      # Generate summary\n    summary_ids = bart_model.generate(\n        inputs['input_ids'], \n        max_length=num_words, \n        num_beams=4, \n        early_stopping=True\n    )\n    \n    # Decode and return the summary\n    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:19:40.435547Z","iopub.execute_input":"2024-12-16T14:19:40.435785Z","iopub.status.idle":"2024-12-16T14:19:40.441003Z","shell.execute_reply.started":"2024-12-16T14:19:40.435761Z","shell.execute_reply":"2024-12-16T14:19:40.440190Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# User input for testing\nuser_input = input(\"Enter the text to summarize: \")\nnum_words = 50\nsummary = summarize_text(user_input, num_words)\nprint(\"Input Text:\", user_input)\nprint(\"Generated Summary:\", summary)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:19:40.441997Z","iopub.execute_input":"2024-12-16T14:19:40.442494Z","iopub.status.idle":"2024-12-16T14:23:00.094993Z","shell.execute_reply.started":"2024-12-16T14:19:40.442466Z","shell.execute_reply":"2024-12-16T14:23:00.094146Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the text to summarize:  Artificial intelligence (AI) has become an integral part of modern technology, influencing industries ranging from healthcare to transportation. By leveraging machine learning algorithms, AI systems can analyze large datasets, recognize patterns, and make predictions with remarkable accuracy. In healthcare, for example, AI aids in diagnosing diseases, personalizing treatment plans, and even predicting patient outcomes. Meanwhile, in transportation, self-driving cars and intelligent traffic management systems are reshaping the way people and goods move. Despite its advantages, AI also raises concerns about privacy, job displacement, and ethical considerations, prompting ongoing debates about how best to balance innovation with responsibility.\n"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input Text: Artificial intelligence (AI) has become an integral part of modern technology, influencing industries ranging from healthcare to transportation. By leveraging machine learning algorithms, AI systems can analyze large datasets, recognize patterns, and make predictions with remarkable accuracy. In healthcare, for example, AI aids in diagnosing diseases, personalizing treatment plans, and even predicting patient outcomes. Meanwhile, in transportation, self-driving cars and intelligent traffic management systems are reshaping the way people and goods move. Despite its advantages, AI also raises concerns about privacy, job displacement, and ethical considerations, prompting ongoing debates about how best to balance innovation with responsibility.\nGenerated Summary: Artificial intelligence (AI) has become an integral part of modern technology, influencing industries ranging from healthcare to transportation .\nBy leveraging machine learning algorithms, AI systems can analyze large datasets, recognize patterns, and make predictions with remarkable\n","output_type":"stream"}],"execution_count":15}]}